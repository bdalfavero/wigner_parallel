\documentclass{article}

\title{The Truncated Wigner method on an HPC cluster}
\author{Benjamin DalFavero}
\date{\today}

\usepackage{physics}
\usepackage{graphicx}

\newcommand{\Schroedinger}{Schr{\"o}dinger}

\begin{document}

\maketitle

\section{Introduction} \label{introduction}

Systems of many interacting, bosonic particles are of great interest in the areas of condensed matter physics, 
chemsitry, quantum sensing, and quantum analogues. At thermal equilibrium, these systems can be treated well by 
Markov chain Monte Carlo (MCMC) methods. For systems out of equilibrium, the path integral Monte Carlo approach 
is infeasible due to the famous ``Monte Carlo sign problem,'' where fast fluctuations in the integrand require
an enormous number of samples for convergence. Lagrangian approaches using the path integral must thus be abandoned
in favor of Hamiltonian approaches in the \Schroedinger{} or Heisenberg pictures. With these methods, the most 
fundamental challenge is the exponential scaling of the Hilbert space with the size of the system.

This paper outlines the implementation of the MCMC methods for Truncated Wigner approximation on 
an HPC cluster, with analysis of how performance is improved by using various constructs from distributed
computing. Section \ref{theory-methods} outlines the structure of the theory, as well as MCMC methods for 
these problems. Section \ref{parallel} details the technical aspects of performing the MCMC calculations 
on the HPC system. Section \ref{results} presents an analysis of the speedup provided by these parallel computing 
techniques. Section \ref{conclusion} concludes the paper, presenting an overview of the work and areas of future 
improvement.

\section{Theory and numerical methods} \label{theory-methods}

The methods in this paper use the phase-space formulation to estimate the seminclassical dynamics 
of the Bose-Hubbard model.

\subsection{The Bose-Hubbard model}

The Bose-Hubbard model (BHM) is a foundational model in condensed matter and atomic physics. 
The model consists of an \(n\)-dimensional lattice with \(M\) sites. The lattice sites 
are occupied by \(N\) interacting bosons. A configuration of this system is described by a Fock state
\begin{equation}
    \label{equ:fock-state}
    \ket{n_1 n_2 \ldots n_M}
\end{equation}
where each \(n_i\) is the number of bosons at lattice site \(i\). The state of the quantum system is a 
superposition of these Fock states:
\begin{equation}
    \label{equ:wave-function}
    \ket{\Psi} = \sum_{n_1, \ldots, n_M} c_{n_1 \ldots n_M} \ket{n_1 n_2 \ldots n_M},
\end{equation}
where the coefficients \(c_{n_1 \ldots n_M}\) are complex numbers. 

Linear maps on quantum states of the form in Eq. \ref{equ:wave-function} are generating 
using creation and annihilation operators
\begin{equation}
    b_i^\dagger \ket{n_1 n_2 \ldots n_i \ldots n_M} = \sqrt{n_i + 1} \ket{n_1 n_2 \ldots (n_i + 1) \ldots n_M}
\end{equation}
\begin{equation}
    b_i \ket{n_1 n_2 \ldots n_i \ldots n_M} = \sqrt{n_i} \ket{n_1 n_2 \ldots (n_i - 1) \ldots n_M}
\end{equation}
which map Fock states to other Fock states. The eigenstates of the annihilation operator 
are the coherent states, which have the relation
\begin{equation}
    \label{equ:coherent-state}
    b_i \ket{\beta_1 \ldots \beta_i \ldots \beta_M} = \beta_i \ket{\beta_1 \ldots \beta_i \ldots \beta_M}
\end{equation}
where the variables \(\beta_i\) are complex numbers. 

The dynamics of the system are governed by the \Schroedinger{} equation 
\begin{equation}
    \label{equ:schroedinger}
    i \hbar \frac{\partial}{\partial t} \ket{\Psi(t)} = \hat{H}_{BH} \ket{\Psi(t)},
\end{equation}
where \(\hbar\) is the reduced Planck constant, \(\ket{\Psi}(t)\) is the state of the system at time \(t\),
and \(\hat{H}_{BH}\) is the Bose-Hubbard Hamiltonian:
\begin{equation}
    \hat{H}_{BH} = -t \sum_{i,j \in NN(i)} (b_i^\dagger b_j + \textrm{h.c.}) + g \sum_i b_i^\dagger b_i^\dagger b_i b_i,
\end{equation}
where \(i,j\) denote lattice sites, \(NN(i)\) is the set of nearest neighbors for lattice site \(i\), \(t\) is the hopping
energy which determines the kinetic energy of a boson hopping from site \(i\) to site \(j\), and \(g\) is the strength 
of on-site repulsive interactions.

\subsection{The Truncated Wigner approximation}

The Trucated Wigner approximation (TWA) is a semiclassical approximation of the dynamics of a bosonic system 
given by the \Schroedinger equation. The TWA relies of Wigner's phase space formalism of quantum mechanics to 
build approximations. 

In the phase space formulation, the quantum state in Eq. \ref{equ:wave-function} is instead represented by the 
Wigner function:
\begin{equation}
    \label{equ:wigner-function}
    W(\psi_i, \psi^\ast_i) = \int d\eta d\eta^\ast \bra{\psi - \eta/2} \hat{\rho} \ket{\psi + \eta/2} 
    \exp(-|\psi|^2 - \frac{1}{4}|\eta|^2) \exp(\frac{1}{2} \eta^\ast \psi - \eta \psi^\ast)
\end{equation}
where \(\psi\) and \(\eta\) are vectors of complex variables, and \(\rho\) is the density matrix. 
In the Heisenberg picture, with the phase-space transformation applied, 
the equation of motion for the Wigner function is 
\begin{equation}
    \label{equ:heisenberg-equation}
    i \hbar \frac{\partial \psi_i}{\partial t} = 
    -t\sum_{j \in NN(i)} \psi_j + g \psi_i + g |\psi_i|^2 \psi_i ,
\end{equation}
where \(\psi_i\) is the complex variable representing the annihilation operator at site \(i\).
The expectation value of the population at lattice site \(i\) is given by 
\begin{equation}
    \label{equ:wigner-population}
    n_i(t) = \int \prod_j d\psi_j d\psi_j^\ast W(\psi(0), \psi^\ast(0)) |\psi_i (t)|^2.
\end{equation}

\subsection{Solution by Monte Carlo methods}

The integral in Eqn. \ref{equ:wigner-population} can be evaluated by Monte Carlo sampling. 
Because \(W\) acts like a probability distribution, when it is positive everywhere, we can sample 
the variables \(\psi_1, \ldots, \psi_N\) using importance sampling. If the inital state is a product of 
coherent states, each \(\psi_i\) is samples from a complex, Gaussian distribution. These samples are 
propagated forward in time by solving Eqn. \ref{equ:heisenberg-equation}, and the population is an average of 
\(|\psi_i^{(k)}|^2\), where \(\psi_i^{(k)}\) is the amplitude at site \(i\) in the \(k^\textrm{th}\) sample.

\section{Parallel implementation} \label{parallel}

Since this method involves both random sampling and solving systems of ordinary differential equations, it is 
imperative to parallelize over both types of tasks. 
There are two convenient ways to divide the workload between nodes such a calculation:
\begin{enumerate}
    \item Each node in the cluster is responsible for a certain number of Monte Carlo samples, 
    and it holds the entire ``wave function'' vector for each sample. After each time step, 
    the nodes average their populations together. 
    \item Each node handles a part of the domain, but does the calculation for every sample 
    on the same node. Each time step, the nodes need to share their boundary points with 
    each other to enable time-stepping. 
\end{enumerate}
In an average calculation, we expect that the number of MCMC samples will vastly outnumber of the number of lattice sites. 
So, I have chosen the first option. Additonally, the first strategy should be better from a memory-use perspective. 
If we were to split up the domain, at each timestep we would need to transer two sets of boundary points between 
pairs of nodes. For each pair of nodes, if \(N_S\) samples are taken, we must transfer \(2 N_S\) complex numbers. 
On the one-dimensional lattice, each block of lattice sites has two neighbors, so we must transfer  
\(4 N_S M/ N_N\) complex numbers, where \(M\) is the number of lattice sites, and \(N_N\) is the number of nodes.
A total of \(2 N_N\) messages must be sent. With the strategy of splitting up samples, we need only transfer 
\(N_N M\) complex numbers, since we are averaging over the average of samples for each node. Since we expect 
\(N_S >> N_M\), the first strategy is a winner on small domains.

On each node, the samples are represented by a matrix 
\begin{equation}
    \Psi_{ij} = 
    \begin{bmatrix}
        \psi_1^1 & \ldots & \psi_1^{N_s} \\
        \vdots & \ddots & \vdots \\
        \psi_M^1 & \ldots & \psi_M^{N_s} \\
    \end{bmatrix}
    ,
\end{equation}
where \(\psi_i^k\) is the variable for lattice site \(i\) in the \(k^\textrm{th}\) sample. Each row of this matrix 
represents a lattice site, and each column a specific sample drawn from the Wigner function. At each step of the 
calculation, we propagate each column forward following its Heisenberg equation of motion, 
and then take the average popualtion in each row.

Secondly, we must decide how the time-stepping and population averaging will be done on the single nodes. 
We generally expect that the number of samples taken will be much greater than the number of lattice sites.
Even when using multiple nodes, each node may still handle several hunderds or thousands of samples, 
while we expect only hundreds of lattice sites. As such, I have chosen to assign each thread to a number of samples as
the main method of multithreading. As for iteration over the lattice sites, I implement two methods for time steps:
\begin{enumerate}
    \item A single loop is performed over the lattice sites.
    \item The potential term \(g |\psi_i|^2 \psi_i\) is added to the new field using OpenMP's SIMD instructions, 
    and then the diffusion term \(-t \sum_{j \in NN(i)} \psi_j + g \psi_i\) is handled by a conventional loop.
\end{enumerate}
I anticipate that parallelizing both the iteration over samples and the iteration over lattice sites may be inefficient, 
since that would require spawning a very large number of threads. However, the SIMD method may be able to speed up
time-stepping without using additional threads. During the averaging portion of the calculation, I compute a squared norm of 
the \(i^\textrm{th}\) row of the sample matrix, which is equivalent to an average when divided by the 
total number of samples. The loop over the lattice sites is easily parallelized.

Memory management is a minimal concern for multithreading. We store two copies of the field, one ``new'' and one ``old.''
At each time step, we update the new field using the old. No extra memory is allocated. Since we only read from the 
old field and write to the new one, no race condition exists. 

\section{Results} \label{results}

\subsection{Convergence and verification}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/convergence.png}
    \caption{Error vs. number of samples, with linear fit. This convergence study was done with 
    4 nodes and 4 threads per process.}
    \label{fig:convergence}
\end{figure}

We must verify that the solution generated by the code is converges to the correct solution as the number of samples 
becomes very large. Since there is no exact solution for this system, we use a ``self-convergence test,'' where the solution 
for a large number of samples is taken as the exact solution. The results of this study are shown in Fig. \ref{fig:convergence}.
Fitting a line on a log-log scale, we see that the slope is about 0.47. This is a good match for the theoretical 
convergence rate, which states that the errors scale as \(O(1/\sqrt{N})\), where \(N\) is the number of samples. 

We verify next that the output of the code does not change when the number of threads or nodes is changed. 
Figs. \ref{fig:verification-thread-simd-comparison} through \ref{fig:verification-thread-comparison} show the distance
between solutions as given by their norm divided by the number of data points. We can see good agreement, given the 
number of samples is in the thousands. We can conclude that the SIMD and non-SIMD implementations produce the same 
solutions. Fig. \ref{fig:verification-nodes-comparison} shows a similar comparison for the number of nodes, 
with a single thread. We now know that the code converges to a final solution, and that this final solution is
indepedent of the number of threads or nodes.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/verification_thread_simd_case1.png}
    \caption{Distance between SIMD and non-SIMD solutions for various numbers of threads.}
    \label{fig:verification-thread-simd-comparison}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/verification_simd_case1.png}
    \caption{Distance between solutions with different numbers of threads for the SIMD case.}
    \label{fig:verification-simd-comparison}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/verification_threads_case1.png}
    \caption{Distance between solutions with different numbers of threads for the non-SIMD case.}
    \label{fig:verification-thread-comparison}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/verification_nodes_case1.png}
    \caption{Distance between solutions with different numbers of nodes.}
    \label{fig:verification-nodes-comparison}
\end{figure}

\subsection{Speedup from distributed-memory parallelism}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/total_time_nodes.png}
    \caption{Total walltime vs. number of nodes, with linear fits.}
    \label{fig:node-total-walltime}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/times_step_nodes.png}
    \caption{Average time per step and averaging vs. number of nodes, with 
    linear fits}
    \label{fig:node-average-step}
\end{figure}

Now we compare the speedup afforded by using multiple nodes. In Fig. \ref{fig:node-total-walltime}, we see the 
total walltime for the simulation vs. the number of cores. In these runs, no multithreading was used, so 
all speedup is due to dividing the samples among multiple nodes. In half of the runs, all cores are on different nodes,
and for the other half, all cores are on separate nodes. The bottom two curves represent the first problem instance,
and the on-node and off-node cases overlap. The top two curves represent the second problem instance, where they 
again overlap.  So, there is no significant penalty due to the locality of the cores, and the system size also 
has little impact on speedup.  The data in Table \ref{tab:node-slopes} show the slopes of the lines in 
Fig. \ref{fig:node-total-walltime}. Ideally (assuming each batch of samples is done independently), 
we would expect a speedup of \(T_1/N\), where \(N\) is the number of nodes used. We do not achieve this ideal 
scaling due to the overhead of gathering data from each of the nodes, averaging them, and sending them back to each 
of the nodes. 

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Case & Locality & Slope \\
        \hline 
        1 & On-node & -0.297 \\
        1 & Off-node & -0.289 \\
        2 & On-node & -0.296 \\
        2 & Off-node & -0.296 \\
        \hline 
    \end{tabular}
    \caption{Slopes of lines in Fig. \ref{fig:node-total-walltime}.}
    \label{tab:node-slopes}
\end{table}

Next we examine the time spent on time-stepping and averaging separately. Average times for these operations are shown in 
Fig. \ref{fig:node-average-vs-step}. Time-sepping is seen to take more time, which is sensible, because 
more explicit iteration is needed for this operation. Averaging can largely be done by computing a vector 
norm of each row in the state vector matrix, and the library implementation is most likely well-optimized. 
The averaging time across multiple nodes could be improved by direct memory access techniques.

Because we do not multithread in these experiments, we expect the time-stepping and averaging times to 
increase linearly with the size of lattice. In Fig. \ref{fig:node-average-vs-step}, we see the order of magnitude
difference in lattice sizes causes an order-of-magnitude (in base 10) difference in the times, which 
supports this scaling.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/node_step_averaging.png}
    \caption{Average time for averaging or time-stepping vs. the number of nodes.}
    \label{fig:node-average-vs-step}
\end{figure}

\subsection{Speedup from shared-memory parallelism}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/thread_scaling_steps.png}
    \caption{Average time for time stepping and averaging vs. number of threads for SIMD and 
    non-SIMD methods}
    \label{fig:threads-avg-step-time}
\end{figure}

The previous subsection shows the speedup afforded by a pure multiprocessing setup, and this section shows the speedup 
from using multithreading. Fig. \ref{fig:threads-avg-step-time} shows the average wallclock time to do a time 
step and averaging as it scales with the number of threads.
Fig. \ref{fig:threads-total-walltime} shows the total walltime for two simulation cases versus the number of threads used.
The scaling is very similar to that presented in Fig. \ref{fig:threads-avg-step-time}.
 In Case 1, the non-SIMD strategy performs marginally better 
for all thread counts. In Case 2, where the lattice is much larger, SIMD performs better when there there are 4 or 
more threads. Since the SIMD strategy requires two loops, larger lattices benefit more because it takes extra time
to set up threads for the two loops, which must be offset by the speedup from the SIMD operations. That being said,
the speedup is still quite small.  

Fig. \ref{fig:threads-averaging-vs-stepping} shows the difference between the time spent on time-stepping and 
that spent on averaging. It can readily be compared with the results in Fig. \ref{fig:node-average-vs-step}, which 
shows an equivalent study that varies the number of nodes. Unlike varying the number of nodes, adding more threads 
does not seems to make averaging more efficient. This is sensible, because in the averagin process, only 
the loop over the individual lattice sites can be parallelized, which is limited by the small number of lattice sites, 
as compared with the cost of spawning the threads. When adding more nodes, on the other hand, we can parallelize over
the samples with little overhead, thus making the operation get faster with the number of nodes. While one could 
try to achieve a similar effect with multithreading, this may be inefficient because of the need to avoid race 
conditions when summing over the large array. Furthermore, the time spent on propagatin the samples is much larger
than that spent on averaging, so improvements need to be concentrated there. This behavior is consistent across
studies where the number of nodes and the number of threads are varied.

A fundamental limitation on these techniques is the attempt to use multithreading to accelerate not only time-stepping,
but also iteration over the independent samples. With either strategy, going to 16 threads only cuts the time per step 
down by half. While the code is free of race conditions that would slow things down, clearly there is a high overhead 
to spawning joining these threads. Further work may be necessary in order to improve this speedup.


\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/threads_total_walltime.png}
    \caption{Total walltime for vs. number of threads for SIMD and non-SIMD methods}
    \label{fig:threads-total-walltime}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{../plots/thread_averaging_vs_stepping.png}
    \caption{Average time spend on each time step or averaging for SIMD and non-SIMD methods.}
    \label{fig:threads-averaging-vs-stepping}
\end{figure}


\section{Conclusion} \label{conclusion}

In this work, we present an implementation of the Trucated Wigner approximation (TWA) on an HPC cluster.
Due to the independent nature of the samples, this method is naturally implemented on a cluster by 
distributing samples across different processors. The implementation of time-stepping also benefits from 
multithreading by allowing multiple samples to be propagated at once, along with the ability to 
implement SIMD computation of kernels.

In section \ref{results}, we show the speedup afforded by these parallel computing mechanisms. 
The use of several nodes shows a power-law decrease in total runtime, but with a non-ideal 
rate of decrease. I speculate that this may be due to the high overhead of averaging samples across nodes, 
but cannot verify this without learning more elaborate profiling tools. A more modest speedup is observed 
from the use of multithreading, both with a conventional loop and SIMD techniques. Again, the overhead of 
the parallelism (in this case spawning and joining threads) seems to reduce the advantage of this technique
from its theoretical speedup. 

The method of splitting the domain of the calculation between nodes was not implemented for this paper. 
In one dimension, we expect a fairly small number of samples with respect to the number of samples, 
so parallelizing over samples is likely more effective. However, in two or more dimensions, domains grow
by higher-degree polynomials. In this case, we might benefit from splitting the domain, as the number of 
samples may become competitive with the number domain points.

A bulk of the total walltime is spent on propagating samples, rather than averaging the population. See 
Figs. \ref{fig:threads-averaging-vs-stepping} and \ref{fig:node-average-vs-step}. Improvements will need 
to be concentrated there. The previously-discussed strategy of parallelizing over the domain may be helpful.
Another way to improve this may be to use co-processor such as a GPU to propagate the fields forward. 
A GPU may allow us to more efficiently implement SIMD kernel and multithreaded iteration over the samples.
However, this approach has a major disadvantage in terms of memory usage and data movement. 
After taking a time step, which may be very efficient on the GPU, we will need to move the new data
into host RAM in order to average across the nodes. This may overwhelm any speedup from the GPU, 
due to the low bandwidth of the processor when copying memory. This can be somewhat mitigated by 
averaging the local samples on the GPU, then moving that averagin into host RAM to average across nodes. 
Futher study of this issue would be needed.

\end{document}